#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VERSIÃ“N MEJORADA DEL SCRIPT ORIGINAL - CORREGIDA
================================================

ðŸ”§ CORRECCIÃ“N APLICADA: Arreglado el bug del MultiIndex de Yahoo Finance
que causaba el error "Not allowed to merge between different levels"

Esta es la versiÃ³n profesional mejorada del script informe_quant_correlation.py
que incluye todas las mejoras solicitadas en una sola herramienta integrada.

MEJORAS IMPLEMENTADAS:
- âœ… ConfiguraciÃ³n matplotlib profesional con fuentes CJK
- âœ… Manejo de errores robusto y logging profesional
- âœ… CÃ³digo modular con funciones especializadas
- âœ… Concurrencia con ThreadPoolExecutor
- âœ… DocumentaciÃ³n completa con docstrings
- âœ… OptimizaciÃ³n de portafolios (Sharpe, min vol, equal-weighted)
- âœ… Frontera eficiente completa
- âœ… AnÃ¡lisis de drawdown avanzado
- âœ… EvoluciÃ³n vs benchmark
- âœ… GrÃ¡ficos individuales de activos
- âœ… MÃ©tricas avanzadas (Sharpe, Sortino, VaR, CVaR)
- âœ… Informe PDF profesional con portada, Ã­ndice, secciones
- âœ… ExportaciÃ³n DOCX para ediciÃ³n
- âœ… Excel mejorado con dashboard
- âœ… Todas las visualizaciones requeridas
- ðŸ”§ BUG CORREGIDO: MultiIndex handling para Yahoo Finance

Autor: Leguillo a.k.a. @leonardoprimero
Fecha: 2025-06-14
VersiÃ³n: 2.1 Professional Edition (Bug Fix)
"""

import os
import sys
import warnings
import logging
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple, Optional

# ConfiguraciÃ³n de warnings y logging
warnings.filterwarnings('ignore', category=FutureWarning)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# =====================================================================================
# CONFIGURACIÃ“N GLOBAL MEJORADA
# =====================================================================================

# Directorios - Adaptable al script original
CARPETA_DATOS = './datospython1'  # Mismo que el script original
CARPETA_SALIDA = './resultados_profesionales'
BENCHMARK = 'SPY'
ROLLING_WINDOW = 60
MIN_DIAS = 200
FECHA_ANALISIS = datetime.now()

# Crear directorios mejorados
for carpeta in [CARPETA_SALIDA, './charts_profesionales', './logs']:
    os.makedirs(carpeta, exist_ok=True)

# =====================================================================================
# CONFIGURACIÃ“N MATPLOTLIB PROFESIONAL
# =====================================================================================

def setup_matplotlib_for_plotting():
    """
    ConfiguraciÃ³n profesional de matplotlib para grÃ¡ficos de alta calidad.
    Implementa soporte para fuentes CJK y configuraciÃ³n no interactiva.
    
    MEJORA: Esta funciÃ³n resuelve problemas de fuentes y mejora calidad visual.
    """
    warnings.filterwarnings('default')
    plt.switch_backend("Agg")
    
    # ConfiguraciÃ³n de estilo profesional
    plt.style.use("seaborn-v0_8-whitegrid")
    sns.set_palette("husl")
    
    # ConfiguraciÃ³n de fuentes multiplataforma con soporte CJK
    plt.rcParams["font.sans-serif"] = [
        "Noto Sans CJK SC", "WenQuanYi Zen Hei", "PingFang SC", 
        "Arial Unicode MS", "Hiragino Sans GB", "Arial", "DejaVu Sans"
    ]
    plt.rcParams["axes.unicode_minus"] = False
    plt.rcParams["figure.dpi"] = 300
    plt.rcParams["savefig.dpi"] = 300
    plt.rcParams["savefig.bbox"] = "tight"
    plt.rcParams["figure.figsize"] = (12, 8)
    
    logger.info("âœ… ConfiguraciÃ³n matplotlib profesional aplicada")

# =====================================================================================
# CLASES MEJORADAS PARA ANÃLISIS CUANTITATIVO
# =====================================================================================

class DataLoaderProfessional:
    """
    Cargador de datos mejorado con manejo robusto de errores y concurrencia.
    
    MEJORAS vs script original:
    - Manejo de errores robusto con try-catch especÃ­ficos
    - Logging detallado de cada operaciÃ³n
    - Soporte para mÃºltiples formatos
    - ValidaciÃ³n de calidad de datos
    """
    
    def __init__(self, carpeta_datos: str, min_dias: int = MIN_DIAS):
        self.carpeta_datos = carpeta_datos
        self.min_dias = min_dias
        self.fechas_por_activo = {}
        
    def leer_archivo_robusto(self, ruta: str) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
        """Lee archivo con manejo robusto de errores."""
        nombre = os.path.splitext(os.path.basename(ruta))[0]
        
        try:
            # Detectar formato automÃ¡ticamente
            if ruta.endswith('.csv'):
                df = pd.read_csv(ruta, index_col=0, parse_dates=True)
            elif ruta.endswith(('.xlsx', '.xls')):
                df = pd.read_excel(ruta, index_col=0, parse_dates=True)
            else:
                logger.warning(f"âš ï¸ Formato no reconocido: {ruta}")
                return None, None
            
            # Buscar columna de precios con mÃºltiples variantes
            columnas_validas = [
                'adj close', 'close', 'precio_cierre', 'Adj Close', 
                'Close', 'Precio_Cierre', 'price', 'Price', 'CLOSE'
            ]
            
            for col_objetivo in columnas_validas:
                for col in df.columns:
                    if col.strip().lower() == col_objetivo.lower():
                        df_filtrado = df[[col]].rename(columns={col: nombre})
                        logger.info(f"âœ… Archivo procesado: {nombre} ({len(df_filtrado)} registros)")
                        return df_filtrado, col
            
            logger.warning(f"âš ï¸ No se encontrÃ³ columna de precios vÃ¡lida en {ruta}")
            logger.info(f"   Columnas disponibles: {list(df.columns)}")
            return None, None
            
        except Exception as e:
            logger.error(f"âŒ Error leyendo {ruta}: {e}")
            return None, None
    
    def cargar_datos_con_concurrencia(self) -> pd.DataFrame:
        """
        Carga datos en paralelo para mejorar rendimiento.
        
        MEJORA: Utiliza ThreadPoolExecutor para carga paralela de archivos.
        """
        if not os.path.exists(self.carpeta_datos):
            raise FileNotFoundError(f"Directorio no encontrado: {self.carpeta_datos}")
        
        archivos = [f for f in os.listdir(self.carpeta_datos) 
                   if not f.startswith('.') and f.split('.')[0].upper() != BENCHMARK.upper()]
        
        if not archivos:
            raise FileNotFoundError("No se encontraron archivos de datos")
        
        dataframes_validos = []
        
        # MEJORA: Carga paralela con ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_archivo = {
                executor.submit(self.leer_archivo_robusto, os.path.join(self.carpeta_datos, archivo)): archivo
                for archivo in archivos
            }
            
            for future in as_completed(future_to_archivo):
                archivo = future_to_archivo[future]
                try:
                    df, col = future.result()
                    if df is not None and len(df) >= self.min_dias:
                        dataframes_validos.append(df)
                        self.fechas_por_activo[df.columns[0]] = (
                            df.index.min(), df.index.max(), len(df)
                        )
                    elif df is not None:
                        logger.warning(f"âš ï¸ {archivo} descartado: solo {len(df)} dÃ­as de datos")
                except Exception as e:
                    logger.error(f"âŒ Error procesando {archivo}: {e}")
        
        if not dataframes_validos:
            raise ValueError("No se encontraron archivos vÃ¡lidos con suficientes datos")
        
        # Merge con fechas comunes (inner join)
        df_merged = dataframes_validos[0]
        for df in dataframes_validos[1:]:
            df_merged = df_merged.join(df, how='inner')
        
        logger.info(f"âœ… Datos consolidados: {df_merged.shape[0]} fechas, {df_merged.shape[1]} activos")
        return df_merged

class AnalyzerProfessional:
    """
    Analizador cuantitativo mejorado con mÃ©tricas avanzadas.
    
    MEJORAS vs script original:
    - MÃ©tricas institucionales (Sharpe, Sortino, VaR, CVaR)
    - AnÃ¡lisis de correlaciones avanzado
    - PCA profesional con interpretaciÃ³n
    - Clustering jerÃ¡rquico optimizado
    """
    
    def __init__(self, datos: pd.DataFrame, benchmark: str = BENCHMARK):
        self.datos = datos
        self.benchmark = benchmark
        self.activos = [col for col in datos.columns if col != benchmark]
        self.retornos = np.log(datos / datos.shift(1)).dropna()
        
        # MÃ©tricas bÃ¡sicas
        self.metricas = {}
        self.correlaciones = {}
        self.clusters = {}
        self.pca_results = {}
        
        logger.info(f"ðŸ’¡ Analizador inicializado para {len(self.activos)} activos")
    
    def calcular_metricas_avanzadas(self):
        """
        Calcula mÃ©tricas institucionales avanzadas.
        
        MEJORA: MÃ©tricas no presentes en script original.
        """
        try:
            for activo in self.datos.columns:
                ret_activo = self.retornos[activo]
                precio_activo = self.datos[activo]
                
                # MÃ©tricas bÃ¡sicas
                ret_anual = ret_activo.mean() * 252
                vol_anual = ret_activo.std() * np.sqrt(252)
                sharpe = ret_anual / vol_anual if vol_anual > 0 else 0
                
                # Sortino ratio
                downside_dev = ret_activo[ret_activo < 0].std() * np.sqrt(252)
                sortino = ret_anual / downside_dev if downside_dev > 0 else 0
                
                # VaR y CVaR al 95%
                var_95 = np.percentile(ret_activo, 5)
                cvar_95 = ret_activo[ret_activo <= var_95].mean()
                
                # Drawdown
                cumulative = (1 + ret_activo).cumprod()
                rolling_max = cumulative.expanding().max()
                drawdown = (cumulative - rolling_max) / rolling_max
                max_drawdown = drawdown.min()
                
                # Beta (solo para activos no benchmark)
                beta = np.nan
                if activo != self.benchmark and self.benchmark in self.retornos.columns:
                    cov = np.cov(ret_activo, self.retornos[self.benchmark])[0, 1]
                    var_bench = np.var(self.retornos[self.benchmark])
                    beta = cov / var_bench if var_bench > 0 else np.nan
                
                self.metricas[activo] = {
                    'retorno_anual': ret_anual,
                    'volatilidad_anual': vol_anual,
                    'sharpe_ratio': sharpe,
                    'sortino_ratio': sortino,
                    'var_95': var_95,
                    'cvar_95': cvar_95,
                    'max_drawdown': max_drawdown,
                    'beta': beta
                }
            
            logger.info("âœ… MÃ©tricas avanzadas calculadas")
            
        except Exception as e:
            logger.error(f"âŒ Error calculando mÃ©tricas: {e}")
            raise
    
    def analisis_correlaciones_completo(self):
        """
        AnÃ¡lisis completo de correlaciones Pearson y Spearman.
        
        MEJORA: AnÃ¡lisis mÃ¡s profundo vs script original.
        """
        try:
            # Correlaciones Pearson y Spearman
            self.correlaciones['pearson'] = self.retornos.corr()
            self.correlaciones['spearman'] = self.retornos.corr(method='spearman')
            
            # Rolling correlations para pares mÃ¡s correlacionados
            corr_flat = self.correlaciones['pearson'].where(
                ~np.eye(self.correlaciones['pearson'].shape[0], dtype=bool)
            ).abs().unstack().sort_values(ascending=False)
            
            top_pairs = []
            for (a1, a2) in corr_flat.index:
                if a1 != a2 and (a2, a1) not in top_pairs:
                    top_pairs.append((a1, a2))
                if len(top_pairs) == 3:  # Top 3 pares
                    break
            
            self.correlaciones['rolling_pairs'] = {}
            for a1, a2 in top_pairs:
                self.correlaciones['rolling_pairs'][f'{a1}-{a2}'] = \
                    self.retornos[a1].rolling(ROLLING_WINDOW).corr(self.retornos[a2])
            
            logger.info("âœ… AnÃ¡lisis de correlaciones completado")
            
        except Exception as e:
            logger.error(f"âŒ Error en anÃ¡lisis de correlaciones: {e}")
            raise
    
    def analisis_pca_avanzado(self):
        """
        AnÃ¡lisis PCA profesional con interpretaciÃ³n de componentes.
        
        MEJORA: PCA mÃ¡s detallado vs script original.
        """
        try:
            from sklearn.decomposition import PCA
            
            # Preparar datos para PCA
            retornos_pca = self.retornos.dropna(axis=1, how='any')
            if retornos_pca.empty:
                logger.warning("âš ï¸ No hay datos suficientes para PCA")
                return
            
            # Ejecutar PCA
            pca = PCA()
            pca.fit(retornos_pca)
            
            # Guardar resultados
            self.pca_results = {
                'explained_variance_ratio': pca.explained_variance_ratio_,
                'components': pd.DataFrame(
                    pca.components_,
                    columns=retornos_pca.columns,
                    index=[f'PC{i+1}' for i in range(pca.components_.shape[0])]
                ),
                'cumulative_variance': np.cumsum(pca.explained_variance_ratio_)
            }
            
            logger.info(f"âœ… PCA completado: {len(pca.explained_variance_ratio_)} componentes")
            
        except Exception as e:
            logger.error(f"âŒ Error en PCA: {e}")
            self.pca_results = {}
    
    def clustering_jerarquico(self):
        """
        Clustering jerÃ¡rquico basado en correlaciones.
        
        MEJORA: Clustering mÃ¡s sofisticado vs script original.
        """
        try:
            from scipy.cluster.hierarchy import linkage, fcluster
            
            # Matriz de distancias basada en correlaciones
            distance_matrix = 1 - self.correlaciones['pearson'].abs()
            
            # Clustering jerÃ¡rquico
            linkage_matrix = linkage(distance_matrix, method='ward')
            
            # Crear clusters
            n_clusters = min(4, len(self.activos) // 2)
            cluster_labels = fcluster(linkage_matrix, t=n_clusters, criterion='maxclust')
            
            self.clusters = {
                'linkage_matrix': linkage_matrix,
                'labels': dict(zip(self.correlaciones['pearson'].columns, cluster_labels)),
                'distance_matrix': distance_matrix
            }
            
            logger.info(f"âœ… Clustering completado: {n_clusters} clusters")
            
        except Exception as e:
            logger.error(f"âŒ Error en clustering: {e}")
            self.clusters = {}

class PortfolioOptimizerProfessional:
    """
    Optimizador de portafolios con algoritmos reales de optimizaciÃ³n.
    
    MEJORAS vs script original:
    - OptimizaciÃ³n matemÃ¡tica real usando scipy
    - MÃºltiples objetivos (Sharpe, mÃ­nima volatilidad, equal-weighted)
    - Frontera eficiente completa
    - ValidaciÃ³n de restricciones
    """
    
    def __init__(self, retornos: pd.DataFrame, benchmark: str = BENCHMARK):
        self.retornos = retornos
        self.benchmark = benchmark
        self.activos = [col for col in retornos.columns if col != benchmark]
        
        # ParÃ¡metros para optimizaciÃ³n
        self.ret_esperados = retornos[self.activos].mean() * 252  # Anualizado
        self.cov_matrix = retornos[self.activos].cov() * 252  # Anualizada
        
        # Portafolios optimizados
        self.portafolios = {}
        self.frontera_eficiente = {}
        
        logger.info(f"ðŸ’¼ Optimizador inicializado para {len(self.activos)} activos")
    
    def optimizar_sharpe_profesional(self) -> Dict:
        """
        OptimizaciÃ³n real del Sharpe ratio usando scipy.optimize.
        
        MEJORA: OptimizaciÃ³n matemÃ¡tica real vs pesos iguales.
        """
        try:
            from scipy.optimize import minimize
            
            n_activos = len(self.activos)
            
            def objetivo_sharpe(weights):
                """FunciÃ³n objetivo: maximizar Sharpe ratio."""
                ret_port = np.sum(self.ret_esperados * weights)
                vol_port = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))
                return -ret_port / vol_port if vol_port > 0 else -999
            
            # Restricciones: suma de pesos = 1
            constraints = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}
            bounds = tuple((0, 1) for _ in range(n_activos))
            
            # OptimizaciÃ³n
            result = minimize(
                objetivo_sharpe,
                x0=np.array([1/n_activos] * n_activos),
                method='SLSQP',
                bounds=bounds,
                constraints=constraints,
                options={'maxiter': 1000}
            )
            
            if result.success:
                weights = result.x
                ret_port = np.sum(self.ret_esperados * weights)
                vol_port = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))
                sharpe_port = ret_port / vol_port
                
                portfolio = {
                    'weights': dict(zip(self.activos, weights)),
                    'expected_return': ret_port,
                    'volatility': vol_port,
                    'sharpe_ratio': sharpe_port,
                    'optimization_success': True
                }
                
                logger.info(f"âœ… OptimizaciÃ³n Sharpe exitosa: {sharpe_port:.3f}")
            else:
                logger.warning("âš ï¸ OptimizaciÃ³n Sharpe fallÃ³, usando pesos iguales")
                weights = np.array([1/n_activos] * n_activos)
                ret_port = np.sum(self.ret_esperados * weights)
                vol_port = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))
                
                portfolio = {
                    'weights': dict(zip(self.activos, weights)),
                    'expected_return': ret_port,
                    'volatility': vol_port,
                    'sharpe_ratio': ret_port / vol_port,
                    'optimization_success': False
                }
            
            self.portafolios['max_sharpe'] = portfolio
            return portfolio
            
        except Exception as e:
            logger.error(f"âŒ Error en optimizaciÃ³n Sharpe: {e}")
            # Fallback a pesos iguales
            return self.crear_equal_weighted()
    
    def optimizar_minima_volatilidad(self) -> Dict:
        """
        OptimizaciÃ³n para mÃ­nima volatilidad.
        
        MEJORA: Nueva funcionalidad no presente en script original.
        """
        try:
            from scipy.optimize import minimize
            
            n_activos = len(self.activos)
            
            def objetivo_volatilidad(weights):
                """FunciÃ³n objetivo: minimizar volatilidad."""
                return np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))
            
            constraints = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}
            bounds = tuple((0, 1) for _ in range(n_activos))
            
            result = minimize(
                objetivo_volatilidad,
                x0=np.array([1/n_activos] * n_activos),
                method='SLSQP',
                bounds=bounds,
                constraints=constraints,
                options={'maxiter': 1000}
            )
            
            if result.success:
                weights = result.x
                ret_port = np.sum(self.ret_esperados * weights)
                vol_port = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))
                
                portfolio = {
                    'weights': dict(zip(self.activos, weights)),
                    'expected_return': ret_port,
                    'volatility': vol_port,
                    'sharpe_ratio': ret_port / vol_port,
                    'optimization_success': True
                }
                
                logger.info(f"âœ… OptimizaciÃ³n mÃ­nima volatilidad exitosa: {vol_port:.3f}")
            else:
                logger.warning("âš ï¸ OptimizaciÃ³n mÃ­nima volatilidad fallÃ³")
                portfolio = self.crear_equal_weighted()
            
            self.portafolios['min_volatility'] = portfolio
            return portfolio
            
        except Exception as e:
            logger.error(f"âŒ Error en optimizaciÃ³n mÃ­nima volatilidad: {e}")
            return self.crear_equal_weighted()
    
    def crear_equal_weighted(self) -> Dict:
        """
        Portafolio equally weighted.
        
        MEJORA: ImplementaciÃ³n consistente con otros portafolios.
        """
        n_activos = len(self.activos)
        weights = np.array([1/n_activos] * n_activos)
        
        ret_port = np.sum(self.ret_esperados * weights)
        vol_port = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))
        
        portfolio = {
            'weights': dict(zip(self.activos, weights)),
            'expected_return': ret_port,
            'volatility': vol_port,
            'sharpe_ratio': ret_port / vol_port,
            'optimization_success': True
        }
        
        self.portafolios['equal_weighted'] = portfolio
        logger.info("âœ… Portafolio equally weighted creado")
        return portfolio
    
    def calcular_frontera_eficiente(self, n_portfolios: int = 50):
        """
        Calcula la frontera eficiente completa.
        
        MEJORA: Nueva funcionalidad no presente en script original.
        """
        try:
            from scipy.optimize import minimize
            
            # Rangos de retorno para la frontera
            ret_min = self.ret_esperados.min()
            ret_max = self.ret_esperados.max()
            target_returns = np.linspace(ret_min, ret_max, n_portfolios)
            
            efficient_portfolios = []
            
            for target_ret in target_returns:
                def objetivo_vol(weights):
                    return np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))
                
                constraints = [
                    {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},
                    {'type': 'eq', 'fun': lambda x: np.sum(self.ret_esperados * x) - target_ret}
                ]
                bounds = tuple((0, 1) for _ in range(len(self.activos)))
                
                result = minimize(
                    objetivo_vol,
                    x0=np.array([1/len(self.activos)] * len(self.activos)),
                    method='SLSQP',
                    bounds=bounds,
                    constraints=constraints,
                    options={'maxiter': 1000}
                )
                
                if result.success:
                    weights = result.x
                    vol = np.sqrt(np.dot(weights.T, np.dot(self.cov_matrix, weights)))
                    efficient_portfolios.append({
                        'return': target_ret,
                        'volatility': vol,
                        'sharpe': target_ret / vol,
                        'weights': weights
                    })
            
            self.frontera_eficiente = {
                'returns': [p['return'] for p in efficient_portfolios],
                'volatilities': [p['volatility'] for p in efficient_portfolios],
                'sharpe_ratios': [p['sharpe'] for p in efficient_portfolios],
                'portfolios': efficient_portfolios
            }
            
            logger.info(f"âœ… Frontera eficiente calculada: {len(efficient_portfolios)} portafolios")
            
        except Exception as e:
            logger.error(f"âŒ Error calculando frontera eficiente: {e}")
            self.frontera_eficiente = {}

class VisualizadorProfessional:
    """
    Generador de visualizaciones profesionales de alta calidad.
    
    MEJORAS vs script original:
    - Visualizaciones de alta calidad (300 DPI)
    - Paletas de colores profesionales
    - GrÃ¡ficos interactivos y explicativos
    - Consistencia visual en toda la suite
    """
    
    def __init__(self, analyzer: AnalyzerProfessional, optimizer: PortfolioOptimizerProfessional):
        self.analyzer = analyzer
        self.optimizer = optimizer
        self.output_dir = './charts_profesionales'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ConfiguraciÃ³n visual profesional
        self.colores = sns.color_palette("husl", len(analyzer.activos))
        self.figsize_default = (12, 8)
        self.figsize_small = (10, 6)
        
        logger.info(f"ðŸŽ¨ Visualizador inicializado para {len(analyzer.activos)} activos")
    
    def plot_correlacion_matrices(self):
        """VisualizaciÃ³n de matrices de correlaciÃ³n Pearson y Spearman."""
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
            
            # Matriz Pearson
            mask1 = np.triu(np.ones_like(self.analyzer.correlaciones['pearson'], dtype=bool))
            sns.heatmap(
                self.analyzer.correlaciones['pearson'], 
                mask=mask1, annot=True, fmt=".2f", 
                cmap="RdBu_r", center=0, ax=ax1,
                linewidths=0.5, cbar_kws={"shrink": .8}
            )
            ax1.set_title("CorrelaciÃ³n Pearson", fontsize=16, fontweight='bold')
            
            # Matriz Spearman
            mask2 = np.triu(np.ones_like(self.analyzer.correlaciones['spearman'], dtype=bool))
            sns.heatmap(
                self.analyzer.correlaciones['spearman'], 
                mask=mask2, annot=True, fmt=".2f", 
                cmap="RdBu_r", center=0, ax=ax2,
                linewidths=0.5, cbar_kws={"shrink": .8}
            )
            ax2.set_title("CorrelaciÃ³n Spearman", fontsize=16, fontweight='bold')
            
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/correlacion_matrices.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… Matrices de correlaciÃ³n generadas")
            
        except Exception as e:
            logger.error(f"âŒ Error en matrices de correlaciÃ³n: {e}")
    
    def plot_dendrograma_clustering(self):
        """Dendrograma de clustering jerÃ¡rquico."""
        try:
            from scipy.cluster.hierarchy import dendrogram
            
            plt.figure(figsize=self.figsize_default)
            dendrogram(
                self.analyzer.clusters['linkage_matrix'],
                labels=list(self.analyzer.correlaciones['pearson'].columns),
                leaf_rotation=45,
                leaf_font_size=12
            )
            plt.title('Dendrograma de Clustering JerÃ¡rquico', fontsize=16, fontweight='bold')
            plt.xlabel('Activos', fontsize=12)
            plt.ylabel('Distancia', fontsize=12)
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/dendrograma_clustering.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… Dendrograma generado")
            
        except Exception as e:
            logger.error(f"âŒ Error en dendrograma: {e}")
    
    def plot_pca_analysis(self):
        """AnÃ¡lisis PCA completo."""
        try:
            if not self.analyzer.pca_results:
                logger.warning("âš ï¸ No hay resultados PCA para visualizar")
                return
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            # Varianza explicada
            explained_var = self.analyzer.pca_results['explained_variance_ratio']
            ax1.bar(range(1, len(explained_var) + 1), explained_var * 100, 
                   color='steelblue', alpha=0.7)
            ax1.set_xlabel('Componente Principal')
            ax1.set_ylabel('% Varianza Explicada')
            ax1.set_title('Varianza Explicada por Componente')
            ax1.grid(True, alpha=0.3)
            
            # Varianza acumulada
            cum_var = self.analyzer.pca_results['cumulative_variance']
            ax2.plot(range(1, len(cum_var) + 1), cum_var * 100, 
                    marker='o', linewidth=2, markersize=6, color='darkred')
            ax2.axhline(y=80, color='gray', linestyle='--', alpha=0.7, label='80%')
            ax2.axhline(y=90, color='gray', linestyle='--', alpha=0.7, label='90%')
            ax2.set_xlabel('NÃºmero de Componentes')
            ax2.set_ylabel('% Varianza Explicada Acumulada')
            ax2.set_title('Varianza Explicada Acumulada')
            ax2.grid(True, alpha=0.3)
            ax2.legend()
            
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/pca_analysis.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… AnÃ¡lisis PCA generado")
            
        except Exception as e:
            logger.error(f"âŒ Error en anÃ¡lisis PCA: {e}")
    
    def plot_rolling_correlations(self):
        """Correlaciones mÃ³viles de los pares mÃ¡s correlacionados."""
        try:
            rolling_pairs = self.analyzer.correlaciones.get('rolling_pairs', {})
            if not rolling_pairs:
                logger.warning("âš ï¸ No hay correlaciones mÃ³viles para visualizar")
                return
            
            n_pairs = len(rolling_pairs)
            fig, axes = plt.subplots(n_pairs, 1, figsize=(14, 4 * n_pairs))
            if n_pairs == 1:
                axes = [axes]
            
            for i, (pair_name, correlation_series) in enumerate(rolling_pairs.items()):
                axes[i].plot(correlation_series.index, correlation_series.values, 
                           linewidth=2, color=self.colores[i % len(self.colores)])
                axes[i].axhline(y=0, color='gray', linestyle='--', alpha=0.5)
                axes[i].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Alta correlaciÃ³n')
                axes[i].axhline(y=-0.5, color='red', linestyle='--', alpha=0.5)
                axes[i].set_title(f'CorrelaciÃ³n MÃ³vil 60 dÃ­as: {pair_name}', fontweight='bold')
                axes[i].set_ylabel('CorrelaciÃ³n')
                axes[i].grid(True, alpha=0.3)
                axes[i].legend()
            
            axes[-1].set_xlabel('Fecha')
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/rolling_correlations.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… Correlaciones mÃ³viles generadas")
            
        except Exception as e:
            logger.error(f"âŒ Error en correlaciones mÃ³viles: {e}")
    
    def plot_efficient_frontier(self):
        """Frontera eficiente con portafolios Ã³ptimos marcados."""
        try:
            if not self.optimizer.frontera_eficiente:
                logger.warning("âš ï¸ No hay frontera eficiente para visualizar")
                return
            
            plt.figure(figsize=self.figsize_default)
            
            # Frontera eficiente
            vols = self.optimizer.frontera_eficiente['volatilities']
            rets = self.optimizer.frontera_eficiente['returns']
            
            plt.plot(vols, rets, 'b-', linewidth=3, label='Frontera Eficiente', alpha=0.8)
            
            # Marcar portafolios especÃ­ficos
            for nombre, portfolio in self.optimizer.portafolios.items():
                if nombre == 'max_sharpe':
                    plt.scatter(portfolio['volatility'], portfolio['expected_return'], 
                              marker='*', s=300, c='red', label='MÃ¡ximo Sharpe', zorder=5)
                elif nombre == 'min_volatility':
                    plt.scatter(portfolio['volatility'], portfolio['expected_return'], 
                              marker='o', s=200, c='green', label='MÃ­nima Volatilidad', zorder=5)
                elif nombre == 'equal_weighted':
                    plt.scatter(portfolio['volatility'], portfolio['expected_return'], 
                              marker='s', s=200, c='orange', label='Equal Weighted', zorder=5)
            
            # Activos individuales
            for activo in self.analyzer.activos:
                if activo in self.analyzer.metricas:
                    vol = self.analyzer.metricas[activo]['volatilidad_anual']
                    ret = self.analyzer.metricas[activo]['retorno_anual']
                    plt.scatter(vol, ret, marker='o', s=80, alpha=0.6, c='gray')
                    plt.annotate(activo, (vol, ret), xytext=(5, 5), 
                               textcoords='offset points', fontsize=9)
            
            plt.xlabel('Volatilidad Anual', fontsize=12)
            plt.ylabel('Retorno Esperado Anual', fontsize=12)
            plt.title('Frontera Eficiente y Portafolios Ã“ptimos', fontsize=16, fontweight='bold')
            plt.legend(loc='upper left')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/efficient_frontier.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… Frontera eficiente generada")
            
        except Exception as e:
            logger.error(f"âŒ Error en frontera eficiente: {e}")
    
    def plot_portfolio_compositions(self):
        """ComposiciÃ³n de portafolios optimizados."""
        try:
            n_portfolios = len(self.optimizer.portafolios)
            if n_portfolios == 0:
                logger.warning("âš ï¸ No hay portafolios para visualizar")
                return
            
            fig, axes = plt.subplots(1, n_portfolios, figsize=(6 * n_portfolios, 6))
            if n_portfolios == 1:
                axes = [axes]
            
            for i, (nombre, portfolio) in enumerate(self.optimizer.portafolios.items()):
                weights = list(portfolio['weights'].values())
                labels = list(portfolio['weights'].keys())
                
                # Filtrar pesos muy pequeÃ±os para mejor visualizaciÃ³n
                min_weight = 0.01
                filtered_weights = []
                filtered_labels = []
                others_weight = 0
                
                for w, l in zip(weights, labels):
                    if w >= min_weight:
                        filtered_weights.append(w)
                        filtered_labels.append(f"{l}\n{w:.1%}")
                    else:
                        others_weight += w
                
                if others_weight > 0:
                    filtered_weights.append(others_weight)
                    filtered_labels.append(f"Otros\n{others_weight:.1%}")
                
                # GrÃ¡fico de torta
                axes[i].pie(filtered_weights, labels=filtered_labels, autopct='',
                           colors=self.colores[:len(filtered_weights)], startangle=90)
                
                # TÃ­tulo con mÃ©tricas
                title = f"{nombre.replace('_', ' ').title()}\n"
                title += f"Sharpe: {portfolio['sharpe_ratio']:.3f}\n"
                title += f"Ret: {portfolio['expected_return']:.1%} | "
                title += f"Vol: {portfolio['volatility']:.1%}"
                
                axes[i].set_title(title, fontweight='bold', fontsize=10)
            
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/portfolio_compositions.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… Composiciones de portafolios generadas")
            
        except Exception as e:
            logger.error(f"âŒ Error en composiciones de portafolios: {e}")
    
    def plot_cumulative_performance(self):
        """Performance acumulativa vs benchmark."""
        try:
            plt.figure(figsize=(14, 8))
            
            # Performance de activos individuales
            for activo in self.analyzer.activos[:8]:  # Limitar a 8 para claridad
                returns_cum = (1 + self.analyzer.retornos[activo]).cumprod()
                plt.plot(returns_cum.index, returns_cum.values, 
                        label=activo, alpha=0.7, linewidth=1.5)
            
            # Performance del benchmark
            if self.analyzer.benchmark in self.analyzer.retornos.columns:
                benchmark_cum = (1 + self.analyzer.retornos[self.analyzer.benchmark]).cumprod()
                plt.plot(benchmark_cum.index, benchmark_cum.values, 
                        label=f'{self.analyzer.benchmark} (Benchmark)', 
                        color='black', linewidth=3, alpha=0.8)
            
            # Performance de portafolios (simulaciÃ³n simple)
            for nombre, portfolio in self.optimizer.portafolios.items():
                if nombre == 'max_sharpe':
                    # Simular retornos del portafolio
                    portfolio_returns = pd.Series(0, index=self.analyzer.retornos.index)
                    for activo, peso in portfolio['weights'].items():
                        if activo in self.analyzer.retornos.columns:
                            portfolio_returns += peso * self.analyzer.retornos[activo]
                    
                    portfolio_cum = (1 + portfolio_returns).cumprod()
                    plt.plot(portfolio_cum.index, portfolio_cum.values, 
                            label=f'Portafolio {nombre.replace("_", " ").title()}',
                            color='red', linewidth=2.5, linestyle='--')
            
            plt.xlabel('Fecha', fontsize=12)
            plt.ylabel('Valor Acumulado (Base = 1)', fontsize=12)
            plt.title('Performance Acumulativa vs Benchmark', fontsize=16, fontweight='bold')
            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/cumulative_performance.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… Performance acumulativa generada")
            
        except Exception as e:
            logger.error(f"âŒ Error en performance acumulativa: {e}")
    
    def plot_drawdown_evolution(self):
        """EvoluciÃ³n de drawdowns por activo."""
        try:
            n_activos = min(8, len(self.analyzer.activos))  # MÃ¡ximo 8 para claridad
            fig, axes = plt.subplots(2, 4, figsize=(20, 10))
            axes = axes.flatten()
            
            for i, activo in enumerate(self.analyzer.activos[:n_activos]):
                returns = self.analyzer.retornos[activo]
                cumulative = (1 + returns).cumprod()
                rolling_max = cumulative.expanding().max()
                drawdown = (cumulative - rolling_max) / rolling_max
                
                axes[i].fill_between(drawdown.index, drawdown.values, 0, 
                                   alpha=0.3, color='red', label='Drawdown')
                axes[i].plot(drawdown.index, drawdown.values, 
                           color='darkred', linewidth=1.5)
                
                max_dd = drawdown.min()
                axes[i].axhline(y=max_dd, color='red', linestyle='--', 
                              label=f'Max DD: {max_dd:.1%}')
                
                axes[i].set_title(f'{activo}', fontweight='bold')
                axes[i].set_ylabel('Drawdown')
                axes[i].grid(True, alpha=0.3)
                axes[i].legend(fontsize=8)
            
            # Ocultar ejes no utilizados
            for j in range(n_activos, len(axes)):
                axes[j].set_visible(False)
            
            plt.suptitle('EvoluciÃ³n de Drawdowns por Activo', fontsize=16, fontweight='bold')
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/drawdown_evolution.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… EvoluciÃ³n de drawdowns generada")
            
        except Exception as e:
            logger.error(f"âŒ Error en evoluciÃ³n de drawdowns: {e}")
    
    def plot_individual_asset_analysis(self, activo: str):
        """AnÃ¡lisis individual detallado por activo (4 paneles)."""
        try:
            if activo not in self.analyzer.datos.columns:
                logger.warning(f"âš ï¸ Activo {activo} no encontrado")
                return
            
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            
            # Panel 1: EvoluciÃ³n del precio
            precio = self.analyzer.datos[activo]
            ax1.plot(precio.index, precio.values, linewidth=1.5, color='blue')
            ax1.set_title(f'{activo}: EvoluciÃ³n del Precio', fontweight='bold')
            ax1.set_ylabel('Precio')
            ax1.grid(True, alpha=0.3)
            
            # Panel 2: Retornos diarios
            returns = self.analyzer.retornos[activo]
            ax2.plot(returns.index, returns.values, linewidth=0.8, alpha=0.7, color='green')
            ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)
            ax2.axhline(y=returns.std(), color='red', linestyle='--', alpha=0.5, label='+1Ïƒ')
            ax2.axhline(y=-returns.std(), color='red', linestyle='--', alpha=0.5, label='-1Ïƒ')
            ax2.set_title(f'{activo}: Retornos Diarios', fontweight='bold')
            ax2.set_ylabel('Retorno')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # Panel 3: DistribuciÃ³n de retornos
            ax3.hist(returns.values, bins=50, alpha=0.7, color='purple', density=True)
            ax3.axvline(x=returns.mean(), color='red', linestyle='--', label=f'Media: {returns.mean():.4f}')
            ax3.axvline(x=returns.std(), color='orange', linestyle='--', label=f'Std: {returns.std():.4f}')
            ax3.set_title(f'{activo}: DistribuciÃ³n de Retornos', fontweight='bold')
            ax3.set_xlabel('Retorno')
            ax3.set_ylabel('Densidad')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
            
            # Panel 4: AutocorrelaciÃ³n de retornos
            from pandas.plotting import autocorrelation_plot
            autocorrelation_plot(returns, ax=ax4)
            ax4.set_title(f'{activo}: AutocorrelaciÃ³n de Retornos', fontweight='bold')
            ax4.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/analisis_individual_{activo}.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"âœ… AnÃ¡lisis individual {activo} generado")
            
        except Exception as e:
            logger.error(f"âŒ Error en anÃ¡lisis individual {activo}: {e}")
    
    def generar_todas_visualizaciones(self):
        """Genera todas las visualizaciones profesionales."""
        try:
            logger.info("ðŸŽ¨ Generando visualizaciones profesionales...")
            
            # Visualizaciones principales
            self.plot_correlacion_matrices()
            self.plot_dendrograma_clustering()
            self.plot_pca_analysis()
            self.plot_rolling_correlations()
            self.plot_efficient_frontier()
            self.plot_portfolio_compositions()
            self.plot_cumulative_performance()
            self.plot_drawdown_evolution()
            
            # AnÃ¡lisis individuales para cada activo
            for activo in self.analyzer.activos:
                self.plot_individual_asset_analysis(activo)
            
            # GrÃ¡ficos adicionales de mÃ©tricas
            self.plot_metrics_comparison()
            self.plot_beta_analysis()
            self.plot_max_drawdowns()
            
            logger.info(f"âœ… Todas las visualizaciones generadas en {self.output_dir}")
            
        except Exception as e:
            logger.error(f"âŒ Error generando visualizaciones: {e}")
    
    def plot_metrics_comparison(self):
        """ComparaciÃ³n de mÃ©tricas clave entre activos."""
        try:
            if not self.analyzer.metricas:
                logger.warning("âš ï¸ No hay mÃ©tricas para comparar")
                return
            
            # Preparar datos
            activos = list(self.analyzer.metricas.keys())
            sharpe_ratios = [self.analyzer.metricas[a]['sharpe_ratio'] for a in activos]
            returns = [self.analyzer.metricas[a]['retorno_anual'] for a in activos]
            volatilities = [self.analyzer.metricas[a]['volatilidad_anual'] for a in activos]
            
            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
            
            # Sharpe ratios
            bars1 = ax1.bar(activos, sharpe_ratios, color=self.colores[:len(activos)], alpha=0.7)
            ax1.set_title('Sharpe Ratio por Activo', fontweight='bold')
            ax1.set_ylabel('Sharpe Ratio')
            ax1.tick_params(axis='x', rotation=45)
            ax1.grid(True, alpha=0.3)
            
            # Retornos anuales
            bars2 = ax2.bar(activos, [r*100 for r in returns], color=self.colores[:len(activos)], alpha=0.7)
            ax2.set_title('Retorno Anual por Activo', fontweight='bold')
            ax2.set_ylabel('Retorno Anual (%)')
            ax2.tick_params(axis='x', rotation=45)
            ax2.grid(True, alpha=0.3)
            
            # Volatilidades anuales
            bars3 = ax3.bar(activos, [v*100 for v in volatilities], color=self.colores[:len(activos)], alpha=0.7)
            ax3.set_title('Volatilidad Anual por Activo', fontweight='bold')
            ax3.set_ylabel('Volatilidad Anual (%)')
            ax3.tick_params(axis='x', rotation=45)
            ax3.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/metrics_comparison.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… ComparaciÃ³n de mÃ©tricas generada")
            
        except Exception as e:
            logger.error(f"âŒ Error en comparaciÃ³n de mÃ©tricas: {e}")
    
    def plot_beta_analysis(self):
        """AnÃ¡lisis de betas respecto al benchmark."""
        try:
            if not self.analyzer.metricas:
                logger.warning("âš ï¸ No hay mÃ©tricas beta para analizar")
                return
            
            # Filtrar activos con beta vÃ¡lido
            activos_beta = []
            betas = []
            
            for activo, metricas in self.analyzer.metricas.items():
                if activo != self.analyzer.benchmark and not np.isnan(metricas['beta']):
                    activos_beta.append(activo)
                    betas.append(metricas['beta'])
            
            if not activos_beta:
                logger.warning("âš ï¸ No hay betas vÃ¡lidos para visualizar")
                return
            
            plt.figure(figsize=self.figsize_default)
            
            # GrÃ¡fico de barras con lÃ­nea de referencia
            bars = plt.bar(activos_beta, betas, color=self.colores[:len(activos_beta)], alpha=0.7)
            plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Beta = 1 (Benchmark)')
            plt.axhline(y=0.0, color='gray', linestyle='-', alpha=0.5)
            
            # Colorear barras segÃºn beta
            for i, (bar, beta) in enumerate(zip(bars, betas)):
                if beta > 1.2:
                    bar.set_color('red')
                elif beta < 0.8:
                    bar.set_color('green')
                else:
                    bar.set_color('orange')
            
            plt.title(f'AnÃ¡lisis Beta respecto a {self.analyzer.benchmark}', fontsize=16, fontweight='bold')
            plt.ylabel('Beta')
            plt.xlabel('Activos')
            plt.xticks(rotation=45)
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # AÃ±adir etiquetas de valores
            for bar, beta in zip(bars, betas):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                        f'{beta:.2f}', ha='center', va='bottom', fontweight='bold')
            
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/beta_analysis.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… AnÃ¡lisis beta generado")
            
        except Exception as e:
            logger.error(f"âŒ Error en anÃ¡lisis beta: {e}")
    
    def plot_max_drawdowns(self):
        """MÃ¡ximos drawdowns por activo."""
        try:
            if not self.analyzer.metricas:
                logger.warning("âš ï¸ No hay mÃ©tricas de drawdown para visualizar")
                return
            
            activos = list(self.analyzer.metricas.keys())
            max_drawdowns = [abs(self.analyzer.metricas[a]['max_drawdown']) * 100 for a in activos]
            
            plt.figure(figsize=self.figsize_default)
            
            bars = plt.bar(activos, max_drawdowns, color=self.colores[:len(activos)], alpha=0.7)
            
            # Colorear segÃºn severidad del drawdown
            for bar, dd in zip(bars, max_drawdowns):
                if dd > 30:
                    bar.set_color('darkred')
                elif dd > 20:
                    bar.set_color('red')
                elif dd > 10:
                    bar.set_color('orange')
                else:
                    bar.set_color('green')
            
            plt.title('MÃ¡ximo Drawdown por Activo', fontsize=16, fontweight='bold')
            plt.ylabel('MÃ¡ximo Drawdown (%)')
            plt.xlabel('Activos')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)
            
            # LÃ­neas de referencia
            plt.axhline(y=10, color='orange', linestyle='--', alpha=0.5, label='10%')
            plt.axhline(y=20, color='red', linestyle='--', alpha=0.5, label='20%')
            plt.axhline(y=30, color='darkred', linestyle='--', alpha=0.5, label='30%')
            plt.legend()
            
            # Etiquetas de valores
            for bar, dd in zip(bars, max_drawdowns):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        f'{dd:.1f}%', ha='center', va='bottom', fontweight='bold')
            
            plt.tight_layout()
            plt.savefig(f"{self.output_dir}/max_drawdowns.png", dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info("âœ… MÃ¡ximos drawdowns generados")
            
        except Exception as e:
            logger.error(f"âŒ Error en mÃ¡ximos drawdowns: {e}")

class GeneradorInformesProfesional:
    """
    Generador de informes profesionales en mÃºltiples formatos.
    
    MEJORAS vs script original:
    - PDF profesional con portada, Ã­ndice automÃ¡tico, secciones estructuradas
    - ExportaciÃ³n DOCX editable para personalizaciÃ³n
    - Excel con mÃºltiples hojas y formateo profesional
    - IntegraciÃ³n completa de visualizaciones
    - Resumen ejecutivo y conclusiones
    """
    
    def __init__(self, analyzer: AnalyzerProfessional, optimizer: PortfolioOptimizerProfessional):
        self.analyzer = analyzer
        self.optimizer = optimizer
        self.output_dir = CARPETA_SALIDA
        self.charts_dir = './charts_profesionales'
        
        logger.info(f"ðŸ“„ Generador de informes inicializado")
    
    def generar_excel_profesional(self):
        """Genera Excel profesional con mÃºltiples hojas formateadas."""
        try:
            excel_path = os.path.join(self.output_dir, 'Analisis_Cuantitativo_Profesional.xlsx')
            
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                # Hoja 1: Resumen Ejecutivo
                resumen_data = []
                resumen_data.append(['RESUMEN EJECUTIVO DEL ANÃLISIS CUANTITATIVO', ''])
                resumen_data.append(['', ''])
                resumen_data.append(['Fecha de AnÃ¡lisis', FECHA_ANALISIS.strftime('%d/%m/%Y')])
                resumen_data.append(['PerÃ­odo Analizado', f"{self.analyzer.datos.index.min().date()} - {self.analyzer.datos.index.max().date()}"])
                resumen_data.append(['NÃºmero de Activos', len(self.analyzer.activos)])
                resumen_data.append(['Benchmark Utilizado', self.analyzer.benchmark])
                resumen_data.append(['Observaciones Totales', len(self.analyzer.datos)])
                resumen_data.append(['', ''])
                
                # Mejores mÃ©tricas
                if self.analyzer.metricas:
                    best_sharpe = max(self.analyzer.metricas.items(), key=lambda x: x[1]['sharpe_ratio'])
                    worst_drawdown = min(self.analyzer.metricas.items(), key=lambda x: x[1]['max_drawdown'])
                    
                    resumen_data.append(['HALLAZGOS CLAVE', ''])
                    resumen_data.append(['Mejor Sharpe Ratio', f"{best_sharpe[0]}: {best_sharpe[1]['sharpe_ratio']:.3f}"])
                    resumen_data.append(['Peor Drawdown', f"{worst_drawdown[0]}: {worst_drawdown[1]['max_drawdown']:.1%}"])
                
                if 'max_sharpe' in self.optimizer.portafolios:
                    best_portfolio = self.optimizer.portafolios['max_sharpe']
                    resumen_data.append(['Portafolio Ã“ptimo (Sharpe)', f"Ret: {best_portfolio['expected_return']:.1%}, Vol: {best_portfolio['volatility']:.1%}"])
                
                resumen_df = pd.DataFrame(resumen_data, columns=['MÃ©trica', 'Valor'])
                resumen_df.to_excel(writer, sheet_name='Resumen_Ejecutivo', index=False)
                
                # Hoja 2: MÃ©tricas Avanzadas
                if self.analyzer.metricas:
                    metricas_df = pd.DataFrame(self.analyzer.metricas).T
                    metricas_df.round(4).to_excel(writer, sheet_name='Metricas_Avanzadas')
                
                # Hoja 3: Correlaciones Pearson
                if 'pearson' in self.analyzer.correlaciones:
                    self.analyzer.correlaciones['pearson'].round(4).to_excel(writer, sheet_name='Correlacion_Pearson')
                
                # Hoja 4: Correlaciones Spearman
                if 'spearman' in self.analyzer.correlaciones:
                    self.analyzer.correlaciones['spearman'].round(4).to_excel(writer, sheet_name='Correlacion_Spearman')
                
                # Hoja 5: PCA
                if self.analyzer.pca_results:
                    pca_var_df = pd.DataFrame(
                        self.analyzer.pca_results['explained_variance_ratio'],
                        columns=['Varianza_Explicada'],
                        index=[f'PC{i+1}' for i in range(len(self.analyzer.pca_results['explained_variance_ratio']))]
                    )
                    pca_var_df.to_excel(writer, sheet_name='PCA_Varianza')
                    
                    if 'components' in self.analyzer.pca_results:
                        self.analyzer.pca_results['components'].T.round(4).to_excel(writer, sheet_name='PCA_Loadings')
                
                # Hoja 6: Portafolios Optimizados
                if self.optimizer.portafolios:
                    portfolios_data = []
                    for nombre, portfolio in self.optimizer.portafolios.items():
                        portfolios_data.append([
                            nombre.replace('_', ' ').title(),
                            f"{portfolio['expected_return']:.1%}",
                            f"{portfolio['volatility']:.1%}",
                            f"{portfolio['sharpe_ratio']:.3f}",
                            portfolio['optimization_success']
                        ])
                    
                    portfolios_df = pd.DataFrame(portfolios_data, 
                                               columns=['Portafolio', 'Retorno_Esperado', 'Volatilidad', 'Sharpe_Ratio', 'Optimizacion_Exitosa'])
                    portfolios_df.to_excel(writer, sheet_name='Portafolios_Optimizados', index=False)
                    
                    # ComposiciÃ³n detallada de cada portafolio
                    for nombre, portfolio in self.optimizer.portafolios.items():
                        weights_df = pd.DataFrame(list(portfolio['weights'].items()), columns=['Activo', 'Peso'])
                        weights_df['Peso_Porcentaje'] = weights_df['Peso'] * 100
                        sheet_name = f"Pesos_{nombre}"[:31]  # LÃ­mite de caracteres en Excel
                        weights_df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # Hoja 7: Datos Raw
                self.analyzer.datos.to_excel(writer, sheet_name='Datos_Raw')
            
            logger.info(f"âœ… Excel profesional generado: {excel_path}")
            
        except Exception as e:
            logger.error(f"âŒ Error generando Excel: {e}")
    
    def generar_pdf_profesional(self):
        """Genera PDF profesional con estructura completa."""
        try:
            from fpdf import FPDF
            from datetime import datetime
            
            class PDF(FPDF):
                def header(self):
                    if self.page_no() > 1:  # No header en portada
                        self.set_font('Arial', 'B', 10)
                        self.cell(0, 10, 'AnÃ¡lisis Cuantitativo Profesional', 0, 1, 'C')
                        self.ln(5)
                
                def footer(self):
                    self.set_y(-15)
                    self.set_font('Arial', 'I', 8)
                    self.cell(0, 10, f'PÃ¡gina {self.page_no()}', 0, 0, 'C')
            
            pdf = PDF()
            pdf.set_auto_page_break(auto=True, margin=15)
            
            # PORTADA
            pdf.add_page()
            pdf.set_font('Arial', 'B', 24)
            pdf.ln(40)
            pdf.cell(0, 20, 'ANÃLISIS CUANTITATIVO', 0, 1, 'C')
            pdf.cell(0, 20, 'PROFESIONAL', 0, 1, 'C')
            
            pdf.ln(20)
            pdf.set_font('Arial', '', 16)
            pdf.cell(0, 10, f'PerÃ­odo: {self.analyzer.datos.index.min().date()} - {self.analyzer.datos.index.max().date()}', 0, 1, 'C')
            pdf.cell(0, 10, f'Activos Analizados: {len(self.analyzer.activos)}', 0, 1, 'C')
            pdf.cell(0, 10, f'Benchmark: {self.analyzer.benchmark}', 0, 1, 'C')
            
            pdf.ln(30)
            pdf.set_font('Arial', 'I', 12)
            pdf.cell(0, 10, f'Generado el: {FECHA_ANALISIS.strftime("%d/%m/%Y %H:%M")}', 0, 1, 'C')
            pdf.cell(0, 10, 'AnÃ¡lisis Cuantitativo de Activos Financieros', 0, 1, 'C')
            
            # ÃNDICE
            pdf.add_page()
            pdf.set_font('Arial', 'B', 16)
            pdf.cell(0, 15, 'ÃNDICE', 0, 1, 'C')
            pdf.ln(10)
            
            pdf.set_font('Arial', '', 12)
            secciones = [
                "1. Resumen Ejecutivo",
                "2. AnÃ¡lisis de Correlaciones",
                "3. Clustering JerÃ¡rquico",
                "4. AnÃ¡lisis de Componentes Principales (PCA)",
                "5. AnÃ¡lisis de Riesgo y Beta",
                "6. OptimizaciÃ³n de Portafolios",
                "7. Frontera Eficiente",
                "8. AnÃ¡lisis Individual de Activos",
                "9. Conclusiones y Recomendaciones"
            ]
            
            for seccion in secciones:
                pdf.cell(0, 8, seccion, 0, 1, 'L')
            
            # RESUMEN EJECUTIVO
            pdf.add_page()
            pdf.set_font('Arial', 'B', 16)
            pdf.cell(0, 15, '1. RESUMEN EJECUTIVO', 0, 1, 'L')
            pdf.ln(5)
            
            pdf.set_font('Arial', '', 11)
            resumen_texto = f"""
Este informe presenta un anÃ¡lisis cuantitativo integral de {len(self.analyzer.activos)} activos financieros 
durante el perÃ­odo {self.analyzer.datos.index.min().date()} a {self.analyzer.datos.index.max().date()}.

OBJETIVOS DEL ANÃLISIS:
â€¢ Evaluar el riesgo y retorno de activos individuales
â€¢ Identificar correlaciones y patrones de mercado
â€¢ Optimizar portafolios segÃºn diferentes criterios
â€¢ Analizar la exposiciÃ³n al riesgo sistemÃ¡tico

HALLAZGOS PRINCIPALES:"""
            
            pdf.multi_cell(0, 6, resumen_texto)
            
            # Agregar hallazgos especÃ­ficos si hay datos
            if self.analyzer.metricas:
                best_sharpe = max(self.analyzer.metricas.items(), key=lambda x: x[1]['sharpe_ratio'])
                worst_drawdown = min(self.analyzer.metricas.items(), key=lambda x: x[1]['max_drawdown'])
                
                pdf.ln(5)
                pdf.multi_cell(0, 6, f"""
â€¢ Mejor Sharpe Ratio: {best_sharpe[0]} ({best_sharpe[1]['sharpe_ratio']:.3f})
â€¢ Mayor Drawdown: {worst_drawdown[0]} ({worst_drawdown[1]['max_drawdown']:.1%})
â€¢ Observaciones analizadas: {len(self.analyzer.datos):,}""")
            
            if 'max_sharpe' in self.optimizer.portafolios:
                portfolio = self.optimizer.portafolios['max_sharpe']
                pdf.multi_cell(0, 6, f"""
â€¢ Portafolio Ã“ptimo: Retorno {portfolio['expected_return']:.1%}, 
  Volatilidad {portfolio['volatility']:.1%}, Sharpe {portfolio['sharpe_ratio']:.3f}""")
            
            # SECCIONES CON GRÃFICOS
            secciones_graficos = [
                ("2. ANÃLISIS DE CORRELACIONES", "correlacion_matrices.png", 
                 "Las matrices de correlaciÃ³n Pearson y Spearman muestran las relaciones lineales entre activos."),
                
                ("3. CLUSTERING JERÃRQUICO", "dendrograma_clustering.png",
                 "El dendrograma revela grupos de activos con comportamientos similares."),
                
                ("4. ANÃLISIS PCA", "pca_analysis.png",
                 "El anÃ¡lisis de componentes principales identifica los factores principales de variaciÃ³n."),
                
                ("5. ANÃLISIS BETA", "beta_analysis.png",
                 "Los betas muestran la sensibilidad de cada activo respecto al benchmark."),
                
                ("6. FRONTERA EFICIENTE", "efficient_frontier.png",
                 "La frontera eficiente presenta las combinaciones Ã³ptimas de riesgo-retorno."),
                
                ("7. COMPOSICIÃ“N DE PORTAFOLIOS", "portfolio_compositions.png",
                 "Los portafolios optimizados muestran las asignaciones ideales por activo."),
                
                ("8. PERFORMANCE ACUMULATIVA", "cumulative_performance.png",
                 "La evoluciÃ³n comparativa muestra el desempeÃ±o relativo de activos y portafolios.")
            ]
            
            for titulo, archivo_imagen, descripcion in secciones_graficos:
                pdf.add_page()
                pdf.set_font('Arial', 'B', 14)
                pdf.cell(0, 15, titulo, 0, 1, 'L')
                pdf.ln(5)
                
                pdf.set_font('Arial', '', 11)
                pdf.multi_cell(0, 6, descripcion)
                pdf.ln(10)
                
                imagen_path = os.path.join(self.charts_dir, archivo_imagen)
                if os.path.exists(imagen_path):
                    try:
                        pdf.image(imagen_path, x=15, w=180)
                    except Exception as e:
                        pdf.multi_cell(0, 6, f"[GrÃ¡fico no disponible: {archivo_imagen}]")
                        logger.warning(f"âš ï¸ No se pudo insertar imagen {archivo_imagen}: {e}")
                else:
                    pdf.multi_cell(0, 6, f"[GrÃ¡fico no disponible: {archivo_imagen}]")
            
            # CONCLUSIONES
            pdf.add_page()
            pdf.set_font('Arial', 'B', 16)
            pdf.cell(0, 15, '9. CONCLUSIONES Y RECOMENDACIONES', 0, 1, 'L')
            pdf.ln(5)
            
            pdf.set_font('Arial', '', 11)
            conclusiones_texto = """
PRINCIPALES CONCLUSIONES:

1. DIVERSIFICACIÃ“N: El anÃ¡lisis de correlaciones revela oportunidades de diversificaciÃ³n 
   entre diferentes sectores y clases de activos.

2. RIESGO SISTEMÃTICO: Los betas calculados muestran la exposiciÃ³n al riesgo de mercado 
   de cada activo individual.

3. OPTIMIZACIÃ“N: Los portafolios optimizados demuestran mejoras significativas en la 
   relaciÃ³n riesgo-retorno comparado con estrategias naive.

4. FACTORES PRINCIPALES: El PCA identifica los factores comunes que impulsan los retornos 
   del conjunto de activos.

RECOMENDACIONES:

â€¢ Implementar el portafolio de mÃ¡ximo Sharpe ratio para optimizar retornos ajustados por riesgo
â€¢ Monitorear correlaciones mÃ³viles para ajustes tÃ¡cticos de asignaciÃ³n
â€¢ Considerar rebalanceo periÃ³dico basado en los pesos Ã³ptimos calculados
â€¢ Evaluar inclusiÃ³n de activos adicionales para mayor diversificaciÃ³n

LIMITACIONES:

â€¢ Los resultados se basan en datos histÃ³ricos y no garantizan performance futura
â€¢ Los modelos asumen distribuciones normales que pueden no aplicar en crisis
â€¢ Se requiere monitoreo continuo de cambios en correlaciones y volatilidades
"""
            
            pdf.multi_cell(0, 6, conclusiones_texto)
            
            # Pie de pÃ¡gina final
            pdf.ln(10)
            pdf.set_font('Arial', 'I', 9)
            pdf.multi_cell(0, 5, """
Este anÃ¡lisis ha sido generado mediante herramientas cuantitativas profesionales para proporcionar 
insights basados en datos sobre el comportamiento de los activos financieros analizados. 
Para consultas tÃ©cnicas o actualizaciones del anÃ¡lisis, contacte al equipo de anÃ¡lisis cuantitativo.""")
            
            # Guardar PDF
            pdf_path = os.path.join(self.output_dir, 'Informe_Cuantitativo_Profesional.pdf')
            pdf.output(pdf_path)
            
            logger.info(f"âœ… PDF profesional generado: {pdf_path}")
            
        except Exception as e:
            logger.error(f"âŒ Error generando PDF: {e}")
    
    def generar_docx_editable(self):
        """Genera documento DOCX editable para personalizaciÃ³n posterior."""
        try:
            try:
                from docx import Document
                from docx.shared import Inches
                from docx.enum.text import WD_ALIGN_PARAGRAPH
            except ImportError:
                logger.warning("âš ï¸ python-docx no disponible, saltando generaciÃ³n DOCX")
                return
            
            doc = Document()
            
            # TÃ­tulo principal
            title = doc.add_heading('ANÃLISIS CUANTITATIVO PROFESIONAL', 0)
            title.alignment = WD_ALIGN_PARAGRAPH.CENTER
            
            # InformaciÃ³n bÃ¡sica
            doc.add_paragraph(f'PerÃ­odo de AnÃ¡lisis: {self.analyzer.datos.index.min().date()} - {self.analyzer.datos.index.max().date()}')
            doc.add_paragraph(f'Activos Analizados: {len(self.analyzer.activos)}')
            doc.add_paragraph(f'Benchmark: {self.analyzer.benchmark}')
            doc.add_paragraph(f'Fecha de GeneraciÃ³n: {FECHA_ANALISIS.strftime("%d/%m/%Y")}')
            
            # Resumen ejecutivo
            doc.add_heading('Resumen Ejecutivo', level=1)
            doc.add_paragraph(
                f'Este documento presenta un anÃ¡lisis cuantitativo integral de {len(self.analyzer.activos)} '
                f'activos financieros, incluyendo anÃ¡lisis de correlaciones, optimizaciÃ³n de portafolios, '
                f'y evaluaciÃ³n de riesgo-retorno.'
            )
            
            # SecciÃ³n de mÃ©tricas
            doc.add_heading('MÃ©tricas Principales', level=1)
            if self.analyzer.metricas:
                best_sharpe = max(self.analyzer.metricas.items(), key=lambda x: x[1]['sharpe_ratio'])
                doc.add_paragraph(f'Mejor Sharpe Ratio: {best_sharpe[0]} ({best_sharpe[1]["sharpe_ratio"]:.3f})')
                
                worst_drawdown = min(self.analyzer.metricas.items(), key=lambda x: x[1]['max_drawdown'])
                doc.add_paragraph(f'Mayor Drawdown: {worst_drawdown[0]} ({worst_drawdown[1]["max_drawdown"]:.1%})')
            
            # Portafolios optimizados
            doc.add_heading('Portafolios Optimizados', level=1)
            if self.optimizer.portafolios:
                for nombre, portfolio in self.optimizer.portafolios.items():
                    doc.add_heading(nombre.replace('_', ' ').title(), level=2)
                    doc.add_paragraph(f'Retorno Esperado: {portfolio["expected_return"]:.1%}')
                    doc.add_paragraph(f'Volatilidad: {portfolio["volatility"]:.1%}')
                    doc.add_paragraph(f'Sharpe Ratio: {portfolio["sharpe_ratio"]:.3f}')
                    
                    # Top holdings
                    doc.add_paragraph('Principales Posiciones:')
                    top_weights = sorted(portfolio['weights'].items(), key=lambda x: x[1], reverse=True)[:5]
                    for activo, peso in top_weights:
                        doc.add_paragraph(f'  â€¢ {activo}: {peso:.1%}', style='ListBullet')
            
            # Conclusiones (espacio para editar)
            doc.add_heading('Conclusiones', level=1)
            doc.add_paragraph('[ESPACIO PARA CONCLUSIONES PERSONALIZADAS]')
            doc.add_paragraph('')
            doc.add_paragraph('[ESPACIO PARA RECOMENDACIONES ESPECÃFICAS]')
            
            # Guardar DOCX
            docx_path = os.path.join(self.output_dir, 'Informe_Cuantitativo_Editable.docx')
            doc.save(docx_path)
            
            logger.info(f"âœ… DOCX editable generado: {docx_path}")
            
        except Exception as e:
            logger.error(f"âŒ Error generando DOCX: {e}")
    
    def generar_todos_los_informes(self):
        """Genera todos los formatos de informe."""
        try:
            logger.info("ðŸ“‹ Generando informes profesionales...")
            
            self.generar_excel_profesional()
            self.generar_pdf_profesional()
            self.generar_docx_editable()
            
            logger.info("âœ… Todos los informes generados exitosamente")
            
        except Exception as e:
            logger.error(f"âŒ Error generando informes: {e}")

# =====================================================================================
# FUNCIÃ“N PRINCIPAL MEJORADA
# =====================================================================================

def main_analisis_profesional():
    """
    FunciÃ³n principal que orquesta todo el anÃ¡lisis cuantitativo profesional.
    
    MEJORAS vs script original:
    - Workflow estructurado y robusto
    - Manejo de errores comprehensivo
    - Logging detallado de progreso
    - ValidaciÃ³n de datos en cada etapa
    - IntegraciÃ³n completa de todos los componentes
    """
    try:
        logger.info("ðŸš€ INICIANDO ANÃLISIS CUANTITATIVO PROFESIONAL v2.1")
        logger.info("=" * 80)
        
        # 1. CONFIGURACIÃ“N INICIAL
        setup_matplotlib_for_plotting()
        
        # 2. CARGA DE DATOS
        logger.info("ðŸ“Š Cargando datos financieros con concurrencia...")
        data_loader = DataLoaderProfessional(CARPETA_DATOS, MIN_DIAS)
        df_activos = data_loader.cargar_datos_con_concurrencia()
        
        # 3. BENCHMARK HANDLING (CORREGIDO)
        benchmark_path = os.path.join(CARPETA_DATOS, f"{BENCHMARK}.csv")
        
        if os.path.exists(benchmark_path):
            logger.info(f"âœ… Benchmark {BENCHMARK} encontrado localmente")
            df_benchmark, _ = data_loader.leer_archivo_robusto(benchmark_path)
            
            if df_benchmark is None:
                raise ValueError(f"Error procesando benchmark local {BENCHMARK}")
                
            # Buscar columna vÃ¡lida
            benchmark_data = pd.read_csv(benchmark_path, index_col=0, parse_dates=True)
            bench_col = None
            for col in ['Adj Close', 'Close', BENCHMARK, 'Precio_Cierre']:
                if col in benchmark_data.columns:
                    bench_col = col
                    break
            
            if bench_col:
                df_benchmark = benchmark_data[[bench_col]].rename(columns={bench_col: BENCHMARK})
            else:
                raise ValueError(f"No se encontrÃ³ columna vÃ¡lida en benchmark {BENCHMARK}")
        else:
            logger.info(f"ðŸ“¥ Descargando benchmark {BENCHMARK}...")
            try:
                import yfinance as yf
                data_yf = yf.download(BENCHMARK, start=df_activos.index.min(), 
                                    end=df_activos.index.max(), progress=False)
                
                # ðŸ”§ CORRECCIÃ“N: Manejar MultiIndex de yfinance
                if isinstance(data_yf.columns, pd.MultiIndex):
                    # Aplanar columnas MultiIndex
                    data_yf.columns = data_yf.columns.droplevel(1)
                
                # Seleccionar columna de precios
                if 'Adj Close' in data_yf.columns:
                    df_benchmark = data_yf[['Adj Close']].rename(columns={'Adj Close': BENCHMARK})
                elif 'Close' in data_yf.columns:
                    df_benchmark = data_yf[['Close']].rename(columns={'Close': BENCHMARK})
                else:
                    # Buscar cualquier columna que contenga 'close'
                    close_cols = [col for col in data_yf.columns if 'close' in col.lower()]
                    if close_cols:
                        df_benchmark = data_yf[[close_cols[0]]].rename(columns={close_cols[0]: BENCHMARK})
                    else:
                        raise ValueError(f"No se encontrÃ³ columna de precios en datos de {BENCHMARK}")
                
                # Guardar para futuro uso
                df_benchmark.to_csv(benchmark_path)
                logger.info(f"âœ… Benchmark guardado en {benchmark_path}")
            except Exception as e:
                logger.error(f"âŒ Error descargando benchmark: {e}")
                raise
        
        # 4. CONSOLIDAR DATOS
        df_benchmark_filtrado = df_benchmark[df_benchmark.index.isin(df_activos.index)]
        df_completo = df_activos.join(df_benchmark_filtrado, how='inner')
        
        if len(df_completo) < 50:
            raise ValueError(f"Datos insuficientes despuÃ©s del merge: {len(df_completo)} dÃ­as")
        
        logger.info(f"âœ… Dataset final: {len(df_completo)} observaciones, {len(df_completo.columns)} activos")
        
        # 5. ANÃLISIS CUANTITATIVO
        logger.info("ðŸ”¬ Ejecutando anÃ¡lisis cuantitativo...")
        analyzer = AnalyzerProfessional(df_completo, BENCHMARK)
        analyzer.calcular_metricas_avanzadas()
        analyzer.analisis_correlaciones_completo()
        analyzer.analisis_pca_avanzado()
        analyzer.clustering_jerarquico()
        
        # 6. OPTIMIZACIÃ“N DE PORTAFOLIOS
        logger.info("ðŸ’¼ Optimizando portafolios...")
        optimizer = PortfolioOptimizerProfessional(analyzer.retornos, BENCHMARK)
        optimizer.optimizar_sharpe_profesional()
        optimizer.optimizar_minima_volatilidad()
        optimizer.crear_equal_weighted()
        optimizer.calcular_frontera_eficiente()
        
        # 7. VISUALIZACIONES
        logger.info("ðŸŽ¨ Generando visualizaciones profesionales...")
        visualizador = VisualizadorProfessional(analyzer, optimizer)
        visualizador.generar_todas_visualizaciones()
        
        # 8. INFORMES
        logger.info("ðŸ“„ Generando informes profesionales...")
        generador_informes = GeneradorInformesProfesional(analyzer, optimizer)
        generador_informes.generar_todos_los_informes()
        
        # 9. RESUMEN FINAL
        logger.info("ðŸ“‹ RESUMEN FINAL:")
        logger.info("-" * 50)
        logger.info(f"ðŸ“Š Activos procesados: {len(analyzer.activos)}")
        logger.info(f"ðŸ“… Observaciones: {len(df_completo)}")
        logger.info(f"ðŸ’¼ Portafolios optimizados: {len(optimizer.portafolios)}")
        
        if 'max_sharpe' in optimizer.portafolios:
            best_sharpe = optimizer.portafolios['max_sharpe']
            logger.info(f"ðŸ† Mejor Sharpe ratio: {best_sharpe['sharpe_ratio']:.3f}")
            logger.info(f"   Retorno esperado: {best_sharpe['expected_return']:.1%}")
            logger.info(f"   Volatilidad: {best_sharpe['volatility']:.1%}")
        
        # Listar archivos generados
        archivos_generados = []
        for pattern in ['*.xlsx', '*.pdf', '*.docx']:
            archivos_generados.extend(list(Path(CARPETA_SALIDA).glob(pattern)))
        
        logger.info(f"ðŸ“ Archivos generados en {CARPETA_SALIDA}:")
        for archivo in archivos_generados:
            logger.info(f"   â€¢ {archivo.name}")
        
        charts_generados = list(Path('./charts_profesionales').glob('*.png'))
        logger.info(f"ðŸ“Š {len(charts_generados)} visualizaciones en ./charts_profesionales/")
        
        logger.info("ðŸŽ‰ ANÃLISIS CUANTITATIVO PROFESIONAL COMPLETADO EXITOSAMENTE!")
        logger.info("=" * 80)
        
        return {
            'analyzer': analyzer,
            'optimizer': optimizer,
            'datos_finales': df_completo,
            'archivos_generados': archivos_generados,
            'charts_generados': charts_generados
        }
        
    except Exception as e:
        logger.error(f"âŒ ERROR CRÃTICO: {e}")
        import traceback
        traceback.print_exc()
        raise

# =====================================================================================
# EJECUCIÃ“N PRINCIPAL
# =====================================================================================

if __name__ == "__main__":
    """
    Punto de entrada principal del script mejorado.
    
    MEJORAS IMPLEMENTADAS vs script original:
    âœ… ConfiguraciÃ³n matplotlib profesional con fuentes CJK
    âœ… Manejo de errores robusto con logging detallado
    âœ… CÃ³digo modular organizado en clases especializadas
    âœ… Concurrencia con ThreadPoolExecutor para mejor rendimiento
    âœ… DocumentaciÃ³n completa con docstrings profesionales
    âœ… OptimizaciÃ³n de portafolios real (Sharpe, mÃ­nima volatilidad, equal-weighted)
    âœ… Frontera eficiente completa calculada matemÃ¡ticamente
    âœ… AnÃ¡lisis de drawdown avanzado con evoluciÃ³n temporal
    âœ… EvoluciÃ³n vs benchmark con grÃ¡ficos comparativos
    âœ… GrÃ¡ficos individuales detallados por cada activo
    âœ… MÃ©tricas avanzadas (Sharpe, Sortino, VaR, CVaR, Beta)
    âœ… Informe PDF profesional con portada, Ã­ndice y secciones estructuradas
    âœ… ExportaciÃ³n DOCX editable para personalizaciÃ³n posterior
    âœ… Excel mejorado con mÃºltiples hojas y formato profesional
    âœ… Todas las visualizaciones requeridas en alta calidad (300 DPI)
    âœ… IntegraciÃ³n completa de todos los componentes
    ðŸ”§ BUG CORREGIDO: Manejo del MultiIndex de Yahoo Finance
    
    COMPATIBILIDAD: Mantiene la estructura de directorios del script original
    pero con funcionalidades enormemente expandidas y calidad profesional.
    """
    
    # Verificar dependencias crÃ­ticas
    try:
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        import seaborn as sns
        import scipy
        import sklearn
        from fpdf import FPDF
        logger.info("âœ… Todas las dependencias verificadas")
    except ImportError as e:
        logger.error(f"âŒ Dependencia faltante: {e}")
        logger.info("ðŸ’¡ Instalar con: pip install pandas numpy matplotlib seaborn scipy scikit-learn fpdf2")
        sys.exit(1)
    
    # Ejecutar anÃ¡lisis principal
    resultados = main_analisis_profesional()
    
    # Mensaje final
    print("\n" + "="*80)
    print("ðŸŽ¯ ANÃLISIS CUANTITATIVO PROFESIONAL COMPLETADO")
    print("="*80)
    print(f"ðŸ“ Resultados disponibles en: {CARPETA_SALIDA}")
    print(f"ðŸ“Š Visualizaciones en: ./charts_profesionales/")
    print("ðŸš€ Â¡Listo para impresionar a clientes de alto nivel!")
    print("="*80)
